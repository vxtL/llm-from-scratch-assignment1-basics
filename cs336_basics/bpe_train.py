from tokenizer import train_bpe
import json
import time
import psutil
import os
import sys
from multiprocessing import Pool, Value, Lock

# å…¨å±€å˜é‡ï¼Œç”¨äºå®æ—¶è¿›åº¦è¾“å‡º
progress_counter = Value('i', 0)  # åˆå§‹åŒ–è®¡æ•°å™¨
progress_lock = Lock()  # åˆå§‹åŒ–é”

def train_chunk(chunk_path, vocab_size, special_tokens, chunk_index):
    """
    è®­ç»ƒå•ä¸ªæ•°æ®å—çš„ BPE åˆ†è¯å™¨ã€‚
    """
    print(f"å¼€å§‹è®­ç»ƒå—: {chunk_index + 1} - {chunk_path}")
    vocab, merges = train_bpe(chunk_path, vocab_size, special_tokens)
    print(f"å®Œæˆè®­ç»ƒå—: {chunk_index + 1} - {chunk_path}")
    return vocab, merges

def split_file(input_path, chunk_size_mb, output_dir):
    """
    å°†å¤§æ–‡ä»¶åˆ†å‰²æˆå¤šä¸ªå°å—ã€‚
    """
    chunk_size = chunk_size_mb * 1024 * 1024
    with open(input_path, 'r', encoding='utf-8') as f:
        chunk_number = 0
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            chunk_path = os.path.join(output_dir, f'chunk_{chunk_number}.txt')
            with open(chunk_path, 'w', encoding='utf-8') as chunk_file:
                chunk_file.write(chunk)
            chunk_number += 1
    return chunk_number

def merge_results(results):
    """
    åˆå¹¶å¤šä¸ªæ•°æ®å—çš„è®­ç»ƒç»“æœã€‚
    """
    combined_vocab = {}
    combined_merges = []
    for vocab, merges in results:
        combined_vocab.update(vocab)
        combined_merges.extend(merges)
    return combined_vocab, combined_merges

def update_progress(chunk_index, total_chunks):
    """
    æ›´æ–°è¿›åº¦å¹¶æ‰“å°å®æ—¶è¿›åº¦ã€‚
    """
    with progress_lock:
        progress_counter.value += 1
        print(f"[{progress_counter.value}/{total_chunks}] æ•°æ®å—å·²å®Œæˆ - å®Œæˆç¬¬ {chunk_index} å—", end="\r")

def main():
    # ==================== é…ç½®å‚æ•° ====================
    USE_SAMPLE_MODE = True  # è®¾ç½®ä¸º True å¯ç”¨é‡‡æ ·æ¨¡å¼ï¼ˆæµ‹è¯•ï¼‰ï¼ŒFalse ä¸ºå®Œæ•´è®­ç»ƒ
    SAMPLE_LINES = 5000    # é‡‡æ ·è¡Œæ•°ï¼ˆå»ºè®® 1000-10000 è¡Œï¼Œçº¦ 5-50MBï¼‰
    input_path = 'owt_train_with_special_token.txt'  # å®Œæ•´æ•°æ®è·¯å¾„
    sample_path = 'owt_train-sample.txt'  # é‡‡æ ·æ•°æ®è¾“å‡ºè·¯å¾„ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
    vocab_size = 32000  # ä¿®æ”¹è¯æ±‡è¡¨å¤§å°ä¸º 32,000
    special_tokens = ['<|endoftext|>']  # ä¿ç•™ç‰¹æ®Šæ ‡è®°
    chunk_size_mb = 1  # æ¯ä¸ªæ•°æ®å—çš„å¤§å°ï¼ˆMBï¼‰
    num_processes = 208  # ä½¿ç”¨çš„è¿›ç¨‹æ•°
    # =================================================

    # æ­¥éª¤1ï¼šæ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(input_path):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        print("\nå¯ç”¨æ–‡ä»¶åˆ—è¡¨:")
        os.system("ls -lh data/")
        sys.exit(1)

    # æ­¥éª¤2ï¼šå¦‚æœä½¿ç”¨é‡‡æ ·æ¨¡å¼ï¼Œåˆ›å»ºå°æ–‡ä»¶
    if USE_SAMPLE_MODE:
        print(f"ğŸ§ª é‡‡æ ·æ¨¡å¼å·²å¯ç”¨: è¯»å–å‰ {SAMPLE_LINES} è¡Œ")
        print(f"æ­£åœ¨åˆ›å»ºé‡‡æ ·æ–‡ä»¶: {sample_path}...")
        
        with open(input_path, 'r', encoding='utf-8') as f_in:
            # åªè¯»å–å‰ SAMPLE_LINES è¡Œ
            sample_lines = [next(f_in) for _ in range(SAMPLE_LINES)]
        
        with open(sample_path, 'w', encoding='utf-8') as f_out:
            f_out.writelines(sample_lines)
        
        time.sleep(1)  # ç­‰å¾…1ç§’ï¼Œç¡®ä¿æ–‡ä»¶å®Œå…¨å†™å…¥
        # ä½¿ç”¨é‡‡æ ·æ–‡ä»¶ä½œä¸ºè¾“å…¥
        actual_input_path = sample_path
        print(f"âœ… é‡‡æ ·æ–‡ä»¶åˆ›å»ºå®Œæˆï¼Œå¤§å°: {os.path.getsize(sample_path) / 1024 / 1024:.2f} MB\n")
    else:
        print("ğŸ“¦ å®Œæ•´è®­ç»ƒæ¨¡å¼")
        actual_input_path = input_path

    # æ­¥éª¤3ï¼šåˆ†å‰²æ–‡ä»¶
    output_dir = 'chunksss'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    print("æ­£åœ¨åˆ†å‰²æ–‡ä»¶...")
    num_chunks = split_file(actual_input_path, chunk_size_mb, output_dir)
    print(f"æ–‡ä»¶å·²åˆ†å‰²æˆ {num_chunks} ä¸ªå—ï¼Œæ¯ä¸ªå—çº¦ {chunk_size_mb} MB")

    # æ­¥éª¤4ï¼šå¤šè¿›ç¨‹è®­ç»ƒ
    print("å¼€å§‹å¤šè¿›ç¨‹è®­ç»ƒ...")
    chunk_paths = [os.path.join(output_dir, f'chunk_{i}.txt') for i in range(num_chunks)]

    # ä½¿ç”¨ Pool.map_async å¹¶æ·»åŠ å›è°ƒå‡½æ•°æ›´æ–°è¿›åº¦
    with Pool(processes=num_processes) as pool:
        results = pool.starmap_async(train_chunk, [(path, vocab_size, special_tokens, i) for i, path in enumerate(chunk_paths)], callback=lambda _: update_progress(progress_counter.value, num_chunks))
        results.get()  # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ

    # æ­¥éª¤5ï¼šåˆå¹¶ç»“æœ
    combined_vocab, combined_merges = merge_results(results.get())

    # æ­¥éª¤6ï¼šä¿å­˜ç»“æœ
    print("\næ­£åœ¨ä¿å­˜ç»“æœ...")
    with open('vocab.json', 'w', encoding='utf-8') as f:
        vocab_serializable = {
            str(k): v.decode('utf-8', errors='replace') for k, v in combined_vocab.items()
        }
        json.dump(vocab_serializable, f, ensure_ascii=False, indent=2)

    with open('merges.json', 'w', encoding='utf-8') as f:
        merges_serializable = [
            (p[0].decode('utf-8', errors='replace'), 
             p[1].decode('utf-8', errors='replace'))
            for p in combined_merges
        ]
        json.dump(merges_serializable, f, ensure_ascii=False, indent=2)

    print("è®­ç»ƒå®Œæˆï¼")
    print(f"æ–‡ä»¶å·²ä¿å­˜:")
    print(f"  - vocab.json ({os.path.getsize('vocab.json') / 1024:.2f} KB)")
    print(f"  - merges.json ({os.path.getsize('merges.json') / 1024:.2f} KB)")

    # æ­¥éª¤7ï¼šæ¸…ç†é‡‡æ ·æ–‡ä»¶å’Œæ•°æ®å—
    if USE_SAMPLE_MODE and os.path.exists(sample_path):
        os.remove(sample_path)
        print(f"\nğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶é‡‡æ ·æ–‡ä»¶: {sample_path}")
    for chunk_path in chunk_paths:
        os.remove(chunk_path)
    print(f"ğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶æ•°æ®å—æ–‡ä»¶")

if __name__ == "__main__":
    main()