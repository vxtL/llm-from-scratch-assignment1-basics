# generate_from_checkpoint.py
import torch
import json
import argparse
import sys
from pathlib import Path
from cs336_basics.model import TransformerLM
from cs336_basics.tokenizer import get_tokenizer
from cs336_basics.generate import decode

def infer_model_config(checkpoint_path):
    """
    ä»æ£€æŸ¥ç‚¹æ–‡ä»¶æ™ºèƒ½æ¨æ–­æ¨¡å‹é…ç½®ï¼ˆé€‚é…ä½ çš„è®­ç»ƒå‚æ•°ï¼‰
    """
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    state_dict = checkpoint['model_state_dict']
    
    # ä»token_embeddingsæ¨æ–­
    vocab_size = state_dict['token_embeddings.weight'].shape[0]
    d_model = state_dict['token_embeddings.weight'].shape[1]
    
    # æ¨æ–­å±‚æ•°ï¼ˆä¸ä½ çš„è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰
    num_layers = sum(1 for key in state_dict.keys() 
                     if key.startswith('layers.') and '.ln1.weight' in key)
    
    # æ¨æ–­num_headsï¼šä»ä½ çš„è®­ç»ƒé»˜è®¤å€¼16è°ƒæ•´
    # å¦‚æœd_model=512, num_heads=16 => head_dim=32
    # å¦‚æœd_model=768, num_heads=12 => head_dim=64
    if d_model == 768:
        num_heads = 12
    elif d_model == 512:
        num_heads = 16
    else:
        num_heads = max(8, d_model // 64)  # æ™ºèƒ½æ¨æ–­
    
    # æ¨æ–­d_ffï¼šä»ä½ çš„è®­ç»ƒé»˜è®¤å€¼1344
    d_ff = None
    for key in state_dict:
        if 'layers.0.ffn.w1.weight' in key:
            d_ff = state_dict[key].shape[0]
            break
    
    if d_ff is None:
        d_ff = d_model * 4  # é»˜è®¤å€¼
    
    # ä¸Šä¸‹æ–‡é•¿åº¦ä»positional encodingæˆ–ç¬¬ä¸€å±‚æ¨æ–­
    context_length = 256  # ä½ çš„è®­ç»ƒé»˜è®¤å€¼
    
    config = {
        "vocab_size": vocab_size,
        "context_length": context_length,
        "d_model": d_model,
        "num_layers": num_layers,
        "num_heads": num_heads,
        "d_ff": d_ff,
        "rope_theta": 10000.0,
    }
    
    print("=" * 60)
    print("ä»æ£€æŸ¥ç‚¹è‡ªåŠ¨æ¨æ–­çš„æ¨¡å‹é…ç½®:")
    for k, v in config.items():
        print(f"  {k}: {v}")
    print("=" * 60)
    
    return config

def load_checkpoint_with_config(checkpoint_path, device='auto'):
    """
    åŠ è½½æ£€æŸ¥ç‚¹å¹¶è‡ªåŠ¨æ¨æ–­é…ç½®
    """
    if device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ¨æ–­é…ç½®
    config = infer_model_config(checkpoint_path)
    
    # åˆ›å»ºå¹¶åŠ è½½æ¨¡å‹
    model = TransformerLM(**config)
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ˆè¿­ä»£æ¬¡æ•°: {checkpoint['iteration']}ï¼‰")
    
    return model, config

# åœ¨æ–‡ä»¶ä¸­æ‰¾åˆ°è¿™ä¸ªå‡½æ•°ï¼Œæ›¿æ¢ else åˆ†æ”¯çš„å†…å®¹
def load_tokenizer_from_training(vocab_path, merges_path):
    """
    å…¼å®¹è®­ç»ƒè„šæœ¬çš„åˆ†è¯å™¨åŠ è½½ï¼ˆå¤„ç†ç‰¹æ®Štokenæ ¼å¼ï¼‰
    """
    print(f"ğŸ“¦ æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
    
    with open(vocab_path, 'r', encoding='utf-8') as f:
        vocab_data = json.load(f)
    
    with open(merges_path, 'r', encoding='utf-8') as f:
        merges_data = json.load(f)
    
    # è½¬æ¢æ ¼å¼ï¼ˆå¤„ç†ç‰¹æ®Štokenå¦‚ <|endoftext|>ï¼‰
    vocab_dict = {}
    for v, k in vocab_data.items():
        token_id = int(k)
        if isinstance(v, str):
            if v.startswith('<') and v.endswith('>'):
                # ç‰¹æ®Štokenä¿æŒåŸæ ·
                token_bytes = v.encode('utf-8')
            else:
                # å¤„ç†unicodeè½¬ä¹‰ï¼Œå¤±è´¥æ—¶å›é€€åˆ°åŸå§‹ç¼–ç 
                try:
                    token_bytes = v.encode('utf-8').decode('unicode_escape').encode('utf-8')
                except UnicodeDecodeError:
                    token_bytes = v.encode('utf-8')
        else:
            # å¦‚æœvæ˜¯æ•´æ•°ï¼ˆå­—èŠ‚å€¼ï¼‰ï¼Œç›´æ¥è½¬ä¸ºå­—èŠ‚
            token_bytes = bytes([v]) if isinstance(v, int) else bytes(v)
        vocab_dict[token_id] = token_bytes
    
    # è½¬æ¢mergesï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    merges_list = []
    for pair in merges_data:
        if isinstance(pair, list) and len(pair) == 2:
            first = pair[0].encode('utf-8') if isinstance(pair[0], str) else bytes([pair[0]])
            second = pair[1].encode('utf-8') if isinstance(pair[1], str) else bytes([pair[1]])
            merges_list.append((first, second))
    
    tokenizer = get_tokenizer(vocab_dict, merges_list, special_tokens=["<|endoftext|>"])
    print(f"âœ… åˆ†è¯å™¨åŠ è½½å®Œæˆï¼ˆè¯æ±‡è¡¨å¤§å°: {len(vocab_dict)}ï¼‰")
    
    return tokenizer

def main():
    parser = argparse.ArgumentParser(description="ä»best.ptç”Ÿæˆæ–‡æœ¬")
    
    # è¾“å…¥æ–‡ä»¶ï¼ˆæ ¹æ®ä½ çš„è®­ç»ƒè„šæœ¬ç»“æ„ï¼‰
    parser.add_argument("--checkpoint", type=str, required=True,
                        help="æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆå¦‚ checkpoints/lr_1e-4/best.ptï¼‰")
    parser.add_argument("--vocab", type=str, default="vocab.json",
                        help="vocab.jsonè·¯å¾„")
    parser.add_argument("--merges", type=str, default="merges.json",
                        help="merges.jsonè·¯å¾„")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--prompt", type=str, default="The future of artificial intelligence is",
                        help="æç¤ºè¯")
    parser.add_argument("--temperature", type=float, default=0.8)
    parser.add_argument("--top-p", type=float, default=0.9)
    parser.add_argument("--max-tokens", type=int, default=256)
    parser.add_argument("--device", type=str, default="auto")
    
    # è¾“å‡º
    parser.add_argument("--output", type=str, default=None,
                        help="ä¿å­˜ç»“æœçš„æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # éªŒè¯æ–‡ä»¶
    for path in [args.checkpoint, args.vocab, args.merges]:
        if not Path(path).exists():
            print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ - {path}")
            sys.exit(1)
    
    # åŠ è½½ç»„ä»¶
    model, config = load_checkpoint_with_config(args.checkpoint, args.device)
    tokenizer = load_tokenizer_from_training(args.vocab, args.merges)
    
    # ç”Ÿæˆ
    print(f"\nğŸ“ ç”Ÿæˆå‚æ•°: temp={args.temperature}, top_p={args.top_p}")
    print(f"ğŸ’¬ æç¤ºè¯: '{args.prompt}'")
    print("=" * 80)
    
    with torch.no_grad():
        full_text, generated_ids = decode(
            model=model,
            tokenizer=tokenizer,
            prompt=args.prompt,
            max_tokens=args.max_tokens,
            temperature=args.temperature,
            top_p=args.top_p,
            end_token="<|endoftext|>",
            device=next(model.parameters()).device
        )
    
    print(full_text)
    print("=" * 80)
    
    # ç»Ÿè®¡
    print(f"\nğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
    print(f"- Tokenæ•°é‡: {len(generated_ids)}")
    print(f"- ç”Ÿæˆå­—ç¬¦æ•°: {len(full_text) - len(args.prompt)}")
    print(f"- é‡åˆ°ç»“æŸç¬¦: {'<|endoftext|>' in full_text}")
    
    # ä¿å­˜
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(full_text)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {args.output}")
    
    return full_text

def batch_generate_comparison():
    """
    æ‰¹é‡ç”Ÿæˆå¯¹æ¯”ï¼ˆç”¨äºæ‰¾å‡ºæœ€ä½³å‚æ•°ï¼‰
    """
    # å‚æ•°ç»„åˆå®éªŒ
    configs = [
        {"temp": 0.5, "top_p": 0.95, "desc": "ä¿å®ˆ-é«˜è´¨é‡"},
        {"temp": 0.8, "top_p": 0.9, "desc": "å¹³è¡¡-æ¨è"},
        {"temp": 1.0, "top_p": 0.85, "desc": "åˆ›é€ -å¤šæ ·æ€§"},
    ]
    
    results = []
    for cfg in configs:
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•é…ç½®: {cfg['desc']} (temp={cfg['temp']}, top_p={cfg['top_p']})")
        print('='*60)
        
        # è¿™é‡Œè°ƒç”¨main()æˆ–ç”Ÿæˆå‡½æ•°
        # ä¸ºç®€æ´èµ·è§ï¼Œçœç•¥å…·ä½“å®ç°

if __name__ == "__main__":
    main()